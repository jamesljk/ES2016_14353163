## 文本处理

### 实验目的

*文本处理（分词，词干提取，去停用词，计算词频）*
1. 移除文本中的标点符号并把所有字母转换为小写的
2. 移除常用的单词
3. 词干提取（getStem）
4. 词元化：它的目的是让单词之间的语法符合规范
*在实际分类中，词元化对分类的性能几乎不会产生什么影响。上面的方法有得也不适用于中文的情况。因此词元化不作实现*


----------


### 实验原理

- 1.对指定的纯文本短文，以字符流的方式读取到一个字符串变量中（该字符串变量即为原始短文中的一个单词）
- 2.	将该字符串变量与停用词表中的所有单词进行比较，若果该词在停用词表中出现过则不对其进行统计，否则在对该词进行词干抽取。
- *测试用的停用词表部分示例如下：*
![测试用停用词表_文本处理][1]
- 3.	对单词进行词干抽取，以避免很多在语义上重复的原始单词被分别统计。词干抽取算法采用的是网络上公开的算法： Porter Stemming Algorithm（地址：http://tartarus.org/~martin/PorterStemmer/ ）。
- 4.	经过以上停用词、词干处理后得到的将是实际进行统计的“单词”（此时的“单词”实际上已经是所有具有相同词干的原是单词的统一代表）。
- 5.	对每一个这样的“单词”进行数量(短文中出现的次数)统计，即为本程序的词频统计。
- 6.	输出所有的“单词”统计结果。


----------


### JAVA CODE:
部分核心代码如下：


----------


- *1.主程序，功能：读取纯文本、停用词处理、词干提取*

	void stat(String text){
     char[] w = new char[501];
     WordsTable wt = new WordsTable();
	try{
	        StringReader in = new StringReader(text);
         	while(true)
           {
           	  int ch = in.read();
              if (Character.isLetter((char) ch))
              {
                 int j = 0;
                 while(true)
                 {  ch = Character.toLowerCase((char) ch);//转小写
                    w[j] = (char) ch;
                    if (j < 500) j++;
                    ch = in.read();
                    if (!Character.isLetter((char) ch))
                    {
                       	String word = new String(w,0,j);
						if(!wt.isStopWord(word)){//如果不是停用词，则进行统计
						word = wt.getStem(word);//提取词干
							wt.stat(word);                             
						}						
                       	break;
                    }
                 }
              }
              if (ch < 0) break;
           }
           in.close();
           jTextArea2.setText("═════════════════");
           jTextArea2.append("\n\n下面结果中单词（这里未计停用词表中同形的单词）的总个数为："+wt.getCount());
           jTextArea2.append("\n词频（这里仅计每个单词词根在文中出现的次数）列表如下：");
      	   jTextArea2.append("\n\n═════════════════\n\n\n");
           Iterator iter = wt.getWords();
           while(iter.hasNext()){
				Word wor = (Word)iter.next();
				jTextArea2.append("\n"+wor.getWord()+"     :     "+wor.getCount());
			}
           jTextArea2.append("\n═════════════════");
           
      }catch (Exception e)
       {  
         	System.out.println(e);
       }
  }


----------


- *2.存放“逻辑单词”的表*
	
	public class WordsTable{
	ArrayList<Word> words = new ArrayList<Word>();
	int count=0;
	//处理一个单词，如果向量表中存在则计数加1，否则将其添加到向量表中
	public void stat(String word){
		Word w;
		for(int i=0;i<words.size();i++){
            w = (Word)words.get(i);
			if(word.equals(w.getWord())){
				w.addCount();
				return;//结束
			}
		}
		//单词表中无该单词时则将其添加到表中
		w = new Word(word);
		words.add(w);
		this.count++;
	}	
	//返回向量表
	public Iterator getWords(){
		return this.words.iterator();
	}
	//返回向量表中单词的个数
	public int getCount(){
		return this.count;
	}
	//返回单词的词干
    public String getStem(String word){
			Stemmer s = new Stemmer();
			s.add(word.toCharArray(),word.length());
			s.stem();
			word = s.toString();
			return word;
    }
    //若果停用词表中有该词则返回true,否则返回false
    public boolean isStopWord(String word){
    	String stopWord = "";
    	try{
	    	File f = new File("en_stopWords.txt");
    		FileInputStream in = new FileInputStream(f);//读取停用词表文件
    		BufferedReader r = new BufferedReader(new InputStreamReader(in));
			while((stopWord=r.readLine())!=null){
				if(word.equals(stopWord)){
					return true;
				}
			}
    	}catch(Exception exc){
    		exc.printStackTrace();//调试
    	}
    	return false;
    }
	}


----------

- *3.存放单词的逻辑“单词”的类*

	public class Word{
	String word="";
	int count=0;
	public Word(String word){
		this.word = word;
		this.count = 1;
	}
	public String getWord(){
		return this.word;
	}
	public int getCount(){
		return this.count;
	}
	//该单词的计数加1
	public void addCount(){
		this.count++;
	}
	}


----------


### 实验结果

*测试用java界面，非最终hadoop上运行界面的截图*

![测试用java结果_文本处理][2]


----------


### Experience

  -   本次实验，我们实现的是用hadoop来海量处理文本数据，并把文本数据归类的这样一个功能。在文本处理分类中，核心技术是贝叶斯算法；在hadoop的平台上，核心技术是MapReduce和分布式文件系统HDFS（分别代表了云计算最基本的计算能力和存储能力）
    
    
  -   我在这次实验中主要做得是文本预处理工作，实现把英文文本分词和词干提取。从而为下一步的贝叶斯算法提供训练集作准备。返回的是一个数组（或者向量类型）的文本向量，包含词干和词频。
    
    
  -   如果将文本分词以单独的mapreduce作业实现虽然也可以达到目的，但是由于增加了MapReduce任务，将增加整个作业的处理的周期，而且还会增加很多I/O操作，因而处理效率会降低。因此，我们可以将分词预处理实现为一个辅助的Map过程，将其与核心Map和Reduce过程合并为一个链式的MapReduce任务，从而完成整个作业。
    
    
  -   这样利用MapReduce软件框架实现，和传统的并行处理方法相比，既能缩短并行程序的开发周期，又加快了对同一数据集的处理时间响应。处理数据集的时间也能减少。
    
    
 -   总结来说，基于Hadoop云计算平台进行海量文本数据集的处理，能使解决并行设计中的各种复杂问题，如分布式存储，工作调度，负载均衡，容错处理以及网络通信等大大简化。通过将传统架构的文本处理方法进行MapReduce分布式并行化的移植实现，文本处理速度也得到了很大提高。所以将Hadoop云计算平台应用于海量数据处理是很好的选择！


  [1]: https://raw.githubusercontent.com/jamesljk/ES2016_14353163/master/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86_%E5%9B%BE1.PNG
  [2]: https://raw.githubusercontent.com/jamesljk/ES2016_14353163/master/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86_%E5%9B%BE2.PNG